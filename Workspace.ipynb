{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input, Output, \n",
    "    Dataset, Model, Metrics, HTML, Markdown,Artifact,\n",
    "    InputPath, OutputPath)\n",
    "\n",
    "from typing import Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  base_image=\"capoolebugchat/kws-training:v0.19.0\",\n",
    ")\n",
    "def train(\n",
    "    model_save_bucket: str,\n",
    "    model_save_path: str,\n",
    "    train_config: Dict,\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    config: Output[Artifact],\n",
    "    # results_metrics: Output[Metrics],\n",
    "    # results_html: Output[HTML],\n",
    "    model_summary: Output[Markdown]\n",
    "):\n",
    "  \"\"\"training component for KWS project\n",
    "  Uses KWS training docker image.\n",
    "  - Inputs:\n",
    "    + dataset: dataset Artifact, \n",
    "      containing datashim created PVC to mount to training docker container\n",
    "    + config: dictionary of training configurations\n",
    "  - Outputs: \n",
    "    + model: model Artifact, containing S3_URI to save model \"\"\"\n",
    "  \n",
    "  import os\n",
    "  import logging\n",
    "  import glob\n",
    "  import yaml\n",
    "  \n",
    "  #create empty dataset dir for PVC mounting\n",
    "  os.system(f\"mkdir /workspace/dataset\")\n",
    "\n",
    "  ### Connect this process to Minio for later writing model into Minio\n",
    "  MINIO_SERVICE_HOST=\"minio-service.kubeflow.svc.cluster.local\"\n",
    "  MINIO_SERVICE_PORT=\"9000\"\n",
    "  #TODO: change these to using Kubeflow's Minio Secrets\n",
    "  MINIO_SERVICE_ACCESS_KEY=\"minio\"\n",
    "  MINIO_SERVICE_SECRET_KEY=\"minio123\"\n",
    "  MINIO_SERVICE_SECURITY_OPTION=False\n",
    "  from minio import Minio\n",
    "  minio_client = Minio(\n",
    "    f\"{MINIO_SERVICE_HOST+':'+MINIO_SERVICE_PORT}\",\n",
    "    access_key = MINIO_SERVICE_ACCESS_KEY,\n",
    "    secret_key = MINIO_SERVICE_SECRET_KEY,\n",
    "    secure     = MINIO_SERVICE_SECURITY_OPTION\n",
    "  )\n",
    "  logging.info(f\"Connected to Minio Server at {MINIO_SERVICE_HOST}:{MINIO_SERVICE_PORT}\")\n",
    "\n",
    "  def _dict_to_env(config_dict):\n",
    "    \"\"\"function to write hyperparams into training environment file\n",
    "    - Inputs: dict: Python dictionary to hold configurations\"\"\"\n",
    "    \n",
    "    yaml_f = open(\"/workspace/h_param.yaml\", 'r')\n",
    "    env_f = open(\"/workspace/hparams.env\",'w')\n",
    "    \n",
    "    def _write_param_to_env(param, value, env_f):\n",
    "      \n",
    "      logging.info(f\"{param} = {value}\")\n",
    "      if isinstance(value, str):\n",
    "        env_f.write(f\"{param} = '{value}'\\n\")\n",
    "      else: env_f.write(f\"{param} = {value}\\n\")\n",
    "\n",
    "    default_hparams = yaml.safe_load(yaml_f)\n",
    "    \n",
    "    print(\"Loading hyperparams:\")\n",
    "    \n",
    "    # loading known configurations\n",
    "    for key in default_hparams:\n",
    "      hyperparam = default_hparams[key]\n",
    "      if key in config_dict:\n",
    "        logging.info(f\"Overiding default run parameter: {key}\")\n",
    "        hyperparam = config_dict[key]\n",
    "      _write_param_to_env(key, hyperparam, env_f)\n",
    "      default_hparams[key] = hyperparam\n",
    "    \n",
    "    # doesn't accept unknown configurations\n",
    "    for key in config_dict:\n",
    "      if key not in default_hparams:\n",
    "        logging.warn(f\"Unknown configuration: {key}, unaccepted to training env\")\n",
    "\n",
    "    return default_hparams\n",
    "\n",
    "  def _train():\n",
    "    \"\"\"Training function. Shouldnt be using this but idk what else can we use\"\"\"\n",
    "    \n",
    "    os.system(\"python3 -m kws_streaming.train.model_train_eval ds_tc_resnet --alsologtostderr\")\n",
    "  \n",
    "  def _upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "    \n",
    "    assert os.path.isdir(local_path)\n",
    "    for local_file in glob.glob(local_path + '/**'):\n",
    "      local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "      if not os.path.isfile(local_file):\n",
    "        _upload_local_directory_to_minio(\n",
    "            local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "      else:\n",
    "        remote_path = os.path.join(\n",
    "            minio_path, local_file[1 + len(local_path):])\n",
    "        remote_path = remote_path.replace(os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "        minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "      \n",
    "  \n",
    "  # Initialize artifacts, save basic infos\n",
    "  with open(model.path, 'w') as model_f:\n",
    "    model_f.write(\"Placeholder\")\n",
    "  model.framework = \"tensorflow\"\n",
    "  model.metadata = {\n",
    "    \"version\":\"Undesigned, Unimplemented\",\n",
    "    \"trained_dataset\": dataset.metadata['PVC'],\n",
    "    \"S3_BUCKET\": model_save_bucket,\n",
    "    \"S3_path\":f\"minio://{model_save_bucket}/{model_save_path}\"\n",
    "  }\n",
    "\n",
    "  def _upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "    \n",
    "    assert os.path.isdir(local_path)\n",
    "    for local_file in glob.glob(local_path + '/**'):\n",
    "      local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "      if not os.path.isfile(local_file):\n",
    "        _upload_local_directory_to_minio(\n",
    "            local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "      else:\n",
    "        remote_path = os.path.join(\n",
    "            minio_path, local_file[1 + len(local_path):])\n",
    "        remote_path = remote_path.replace(os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "        minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "  # Training sequence\n",
    "  logging.info(\"Writing parameters into environment\")\n",
    "  # config[\"data_path\"] = \"/workspace/dataset\"\n",
    "  train_config = _dict_to_env(train_config)\n",
    "  logging.info(\"Hyperparameters written\")\n",
    "\n",
    "  logging.info(\"Training commenced. Read logs.\")\n",
    "  _train()\n",
    "  logging.info(\"Training completed.\")\n",
    "  \n",
    "  logging.info(\"Uploading model\")\n",
    "  _upload_local_directory_to_minio(\n",
    "    local_path = \"./train_res/ds_tc_resnet/non_stream\",\n",
    "    bucket_name = model_save_bucket,\n",
    "    minio_path = model_save_path)\n",
    "  logging.info(\"Model uploaded to minio bucket.\")\n",
    "  \n",
    "  with open(os.path.join(train_config[\"train_res\"], \"model_summary.txt\"), 'r') as local_summary_f:\n",
    "    for line in local_summary_f.readlines():\n",
    "      with open(model_summary.path, 'a') as summary_f:\n",
    "        summary_f.write(line)\n",
    "\n",
    "  import json\n",
    "  with open(config.path, 'w') as config_f:\n",
    "    json.dump(train_config, config_f, indent=4)\n",
    "  logging.info(f\"Training finished, check storage at minio://{model_save_bucket}/{model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def ingest_data(\n",
    "    dataset_pvc: str,\n",
    "    input_path: OutputPath(\"\"),\n",
    "    # dataset_uri: str,\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "\n",
    "    import os\n",
    "    os.system(f\"mkdir {dataset.path[:-4]}model\")\n",
    "    os.system(\"export\")\n",
    "    os.system(\"ls /tmp/outputs/dataset\")\n",
    "    os.system(\"ls /tmp/outputs/dataset/data\")\n",
    "    with open(dataset.path, 'w') as dataset_f:\n",
    "        dataset_f.write(\"Placeholder\")\n",
    "    dataset.name = \"KWSDataset\"\n",
    "    dataset.metadata[\"PVC\"] = dataset_pvc\n",
    "    # dataset.metadata[\"URI\"] = dataset_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.train' has no attribute 'summary_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m all_log \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m event_paths:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     log \u001b[39m=\u001b[39m sum_log(path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m log \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39mif\u001b[39;00m all_log\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb Cell 4\u001b[0m in \u001b[0;36msum_log\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum_log\u001b[39m(path):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     runlog \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49msummary_iterator(path):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mvalue:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lebugcat/Desktop/FTech/Projects/KWS-ContinuousTraining/Workspace.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m             r \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m'\u001b[39m: v\u001b[39m.\u001b[39mtag, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m:v\u001b[39m.\u001b[39msimple_value}\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.train' has no attribute 'summary_iterator'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.train import summary_iterator\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Get all event* runs from logging_dir subdirectories\n",
    "logging_dir = ''\n",
    "event_paths = [\"events.out.tfevents.1663056540.LeMonade\"]\n",
    "\n",
    "\n",
    "# Extraction function\n",
    "def sum_log(path):\n",
    "    runlog = pd.DataFrame(columns=['metric', 'value'])\n",
    "    \n",
    "    for e in summary_iterator(path):\n",
    "        for v in e.summary.value:\n",
    "            r = {'metric': v.tag, 'value':v.simple_value}\n",
    "            runlog = runlog.append(r, ignore_index=True)\n",
    "\n",
    "    runlog['epoch'] = [item for sublist in [[i]*5 for i in range(0, len(runlog)//5)] for item in sublist]\n",
    "    \n",
    "    return runlog\n",
    "\n",
    "\n",
    "# Call & append\n",
    "all_log = pd.DataFrame()\n",
    "for path in event_paths:\n",
    "    log = sum_log(path)\n",
    "    if log is not None:\n",
    "        if all_log.shape[0] == 0:\n",
    "            all_log = log\n",
    "        else:\n",
    "            all_log = all_log.append(log)\n",
    "\n",
    "\n",
    "# Inspect\n",
    "print(all_log.shape)\n",
    "all_log.head()    \n",
    "            \n",
    "# Store\n",
    "all_log.to_csv('all_training_logs_in_one_file.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.onprem import mount_pvc\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"KWS Auto Train Pipeline\",\n",
    "    description=\"ultilized for KFP running, in development\"\n",
    ")\n",
    "def pipeline(\n",
    "    train_config: Optional[Dict],\n",
    "    model_bucket: str,\n",
    "    model_path: str\n",
    "):\n",
    "    \n",
    "    #TODO: add validation of data dir\n",
    "    data_ingest_task = ingest_data(\n",
    "        dataset_pvc = \"kws-dataset\"\n",
    "        )\n",
    "    #TODO: do smt with dataset path, since datashim is only able to mount buckets, not path\n",
    "    #TODO: do smt with the mounting location, use some variables fgs\n",
    "    #TODO: add completed run metrics visualization\n",
    "    #TODO: change config into artifact from ingest&validate config for train task consuming\n",
    "    train_task = train(\n",
    "        model_save_bucket = model_bucket,\n",
    "        model_save_path = model_path,\n",
    "        train_config = train_config,\n",
    "        dataset = data_ingest_task.outputs[\"dataset\"]\n",
    "    )\n",
    "\n",
    "    train_task.apply(mount_pvc(\n",
    "        pvc_name = \"kws-dataset\",\n",
    "        volume_name = \"dataset\",\n",
    "        volume_mount_path = \"/workspace/dataset\"\n",
    "    ))\n",
    "\n",
    "from kfp.compiler import Compiler\n",
    "\n",
    "kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import logging\n",
    "# ### Connect this process to Minio for later writing model into Minio\n",
    "# MINIO_SERVICE_HOST=\"192.168.1.22\"\n",
    "# MINIO_SERVICE_PORT=\"9000\"\n",
    "# #TODO: change these to using Kubeflow's Minio Secrets\n",
    "# MINIO_SERVICE_ACCESS_KEY=\"minio\"\n",
    "# MINIO_SERVICE_SECRET_KEY=\"minio123\"\n",
    "# MINIO_SERVICE_SECURITY_OPTION=False\n",
    "# from minio import Minio\n",
    "# minio_client = Minio(\n",
    "# f\"{MINIO_SERVICE_HOST+':'+MINIO_SERVICE_PORT}\",\n",
    "# access_key = MINIO_SERVICE_ACCESS_KEY,\n",
    "# secret_key = MINIO_SERVICE_SECRET_KEY,\n",
    "# secure     = MINIO_SERVICE_SECURITY_OPTION\n",
    "# )\n",
    "# logging.info(f\"Connected to Minio Server at {MINIO_SERVICE_HOST}:{MINIO_SERVICE_PORT}\")\n",
    "\n",
    "# import os\n",
    "# import glob\n",
    "# def _upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "\n",
    "#     assert os.path.isdir(local_path)\n",
    "#     for local_file in glob.glob(local_path + '/**'):\n",
    "#         local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "#         if not os.path.isfile(local_file):\n",
    "#             _upload_local_directory_to_minio(\n",
    "#                 local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "#         else:\n",
    "#             remote_path = os.path.join(\n",
    "#             minio_path, local_file[1 + len(local_path):])\n",
    "#             remote_path = remote_path.replace(os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "#             minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "# _upload_local_directory_to_minio(\n",
    "#     \"components/2_train/local_dataset\",\n",
    "#     \"ftech-ai\",\n",
    "#     \"KWSdataset\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('kws-dev-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a192e72af163c78e5ecb2f39af3cac0b99b94b35df0ed122b563314cb768475"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
